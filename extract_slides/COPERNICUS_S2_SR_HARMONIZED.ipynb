{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbb1482-be59-4f23-8629-2697f3ed50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export tasks have been started.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize the Earth Engine module.\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/orwell2024/uscrnlib/main/extract_slides/2024stations_days.csv'\n",
    "\n",
    "# Read the CSV file from the URL\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Limiting to first 10 locations for testing purposes\n",
    "data = data.head(4)\n",
    "\n",
    "# Function to create and export image for a given location\n",
    "def create_and_export_image(row):\n",
    "    station_name = row['Station']\n",
    "    latitude = row['LATITUDE']\n",
    "    longitude = row['LONGITUDE']\n",
    "    \n",
    "    point = ee.Geometry.Point([longitude, latitude])\n",
    "    buffer = point.buffer(10000).bounds()  # 5000 meters buffer for 10 km x 10 km region\n",
    "    \n",
    "    collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "                    .filterBounds(buffer) \\\n",
    "                    .filterDate('2023-04-11', '2023-6-01') \\\n",
    "                    .sort('CLOUDY_PIXEL_PERCENTAGE') \\\n",
    "                    .first()\n",
    "    \n",
    "    # Get the date of the image\n",
    "    date = ee.Date(collection.get('system:time_start')).format('YYYY-MM-dd').getInfo()\n",
    "    \n",
    "    # Select the relevant bands\n",
    "    image = collection.select(['B4', 'B3', 'B2']).clip(buffer)\n",
    "    \n",
    "    # Export the image with smaller dimensions\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=image,\n",
    "        description=f\"{station_name}_{date}\",\n",
    "        folder='GEE_Images',\n",
    "        scale=10,\n",
    "        region=buffer.getInfo()['coordinates'],\n",
    "        fileFormat='GeoTIFF',  # Export as GeoTIFF\n",
    "        maxPixels=1e8\n",
    "    )\n",
    "    \n",
    "    export_task.start()\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "data.apply(create_and_export_image, axis=1)\n",
    "\n",
    "print(\"Export tasks have been started.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cec0d18-c0cc-4a30-8290-7dddfab72a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export tasks have been started and results have been written to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize the Earth Engine module.\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/orwell2024/uscrnlib/main/extract_slides/2024stations_days.csv'\n",
    "\n",
    "# Read the CSV file from the URL\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Filter the data to include only stations from Alabama (AL)\n",
    "data_al = data[data['Station'].str.startswith('TX_')].copy()\n",
    "\n",
    "# Initialize lists to store results\n",
    "built_1975_percent_list = []\n",
    "built_2020_percent_list = []\n",
    "percentage_change_list = []\n",
    "\n",
    "# Function to create and export image for a given location\n",
    "def process_location(row):\n",
    "    station_name = row['Station']\n",
    "    latitude = row['LATITUDE']\n",
    "    longitude = row['LONGITUDE']\n",
    "    sizeKm = 10  # Size of the cell in kilometers\n",
    "\n",
    "    # Define a point for the center of the rectangle at the specified coordinates\n",
    "    centerPoint = ee.Geometry.Point([longitude, latitude])\n",
    "\n",
    "    # Create a bounding box around the center point\n",
    "    halfSideLength = (sizeKm / 2) * 1000  # Convert km to meters\n",
    "    cell = centerPoint.buffer(halfSideLength).bounds()\n",
    "\n",
    "    # Load the built-up surface images for 1975 and 2020 from the JRC GHSL dataset\n",
    "    image_1975 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/1975').select('built_surface')\n",
    "    image_2020 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/2020').select('built_surface')\n",
    "\n",
    "    # Clip the built-up images to the cell\n",
    "    built_1975_clipped = image_1975.clip(cell)\n",
    "    built_2020_clipped = image_2020.clip(cell)\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 1975\n",
    "    mean1975 = built_1975_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 2020\n",
    "    mean2020 = built_2020_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Normalize to percentage of the area (1% = 10,000 square meters per hectare)\n",
    "    percentage1975 = round((mean1975 / 10000) * 100, 2) if mean1975 is not None else 0\n",
    "    percentage2020 = round((mean2020 / 10000) * 100, 2) if mean2020 is not None else 0\n",
    "\n",
    "    # Calculate the percentage change\n",
    "    percentage_change = round(((percentage2020 - percentage1975) / percentage1975) * 100, 2) if percentage1975 != 0 else None\n",
    "\n",
    "    # Append results to lists\n",
    "    built_1975_percent_list.append(percentage1975)\n",
    "    built_2020_percent_list.append(percentage2020)\n",
    "    percentage_change_list.append(percentage_change)\n",
    "\n",
    "    # Export the 1975 image to Google Drive\n",
    "    export_task_1975 = ee.batch.Export.image.toDrive(\n",
    "        image=built_1975_clipped,\n",
    "        description=f'Built_up_surface_1975_{station_name}_{sizeKm}km_cell',\n",
    "        folder='GEE_Images',\n",
    "        scale=30,\n",
    "        region=cell,\n",
    "        maxPixels=1e9\n",
    "    )\n",
    "    export_task_1975.start()\n",
    "\n",
    "    # Export the 2020 image to Google Drive\n",
    "    export_task_2020 = ee.batch.Export.image.toDrive(\n",
    "        image=built_2020_clipped,\n",
    "        description=f'Built_up_surface_2020_{station_name}_{sizeKm}km_cell',\n",
    "        folder='GEE_Images',\n",
    "        scale=30,\n",
    "        region=cell,\n",
    "        maxPixels=1e9\n",
    "    )\n",
    "    export_task_2020.start()\n",
    "\n",
    "# Apply the function to each row in the filtered dataframe\n",
    "data_al.apply(process_location, axis=1)\n",
    "\n",
    "# Add the results to the dataframe\n",
    "data_al.loc[:, 'Built_1975_percent'] = built_1975_percent_list\n",
    "data_al.loc[:, 'Built_2020_percent'] = built_2020_percent_list\n",
    "data_al.loc[:, 'Percentage_Change'] = percentage_change_list\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_csv_path = 'updated_stations_data_al.csv'\n",
    "data_al.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Export tasks have been started and results have been written to the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060d364e-e2e1-432e-94fc-10f086da9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing is complete and results have been written to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize the Earth Engine module.\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# URL of the CSV file\n",
    "url = 'https://raw.githubusercontent.com/orwell2024/uscrnlib/main/extract_slides/2024stations_days.csv'\n",
    "\n",
    "# Read the CSV file from the URL\n",
    "response = requests.get(url)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Filter the data to include only stations from Texas (TX)\n",
    "data_tx = data#[data['Station'].str.startswith('TX_')].copy()\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = {\n",
    "    \"Built_1975_50km_percent\": [],\n",
    "    \"Built_2020_50km_percent\": [],\n",
    "    \"Percentage_Change_50km\": [],\n",
    "    \"Built_1975_10km_percent\": [],\n",
    "    \"Built_2020_10km_percent\": [],\n",
    "    \"Percentage_Change_10km\": [],\n",
    "    \"Built_1975_2km_percent\": [],\n",
    "    \"Built_2020_2km_percent\": [],\n",
    "    \"Percentage_Change_2km\": []\n",
    "}\n",
    "\n",
    "# Function to process location and calculate built-up surface percentages\n",
    "def process_location(row, sizeKm):\n",
    "    latitude = row['LATITUDE']\n",
    "    longitude = row['LONGITUDE']\n",
    "\n",
    "    # Define a point for the center of the rectangle at the specified coordinates\n",
    "    centerPoint = ee.Geometry.Point([longitude, latitude])\n",
    "\n",
    "    # Create a bounding box around the center point\n",
    "    halfSideLength = (sizeKm / 2) * 1000  # Convert km to meters\n",
    "    cell = centerPoint.buffer(halfSideLength).bounds()\n",
    "\n",
    "    # Load the built-up surface images for 1975 and 2020 from the JRC GHSL dataset\n",
    "    image_1975 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/1975').select('built_surface')\n",
    "    image_2020 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/2020').select('built_surface')\n",
    "\n",
    "    # Clip the built-up images to the cell\n",
    "    built_1975_clipped = image_1975.clip(cell)\n",
    "    built_2020_clipped = image_2020.clip(cell)\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 1975\n",
    "    mean1975 = built_1975_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 2020\n",
    "    mean2020 = built_2020_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Normalize to percentage of the area (1% = 10,000 square meters per hectare)\n",
    "    percentage1975 = round((mean1975 / 10000) * 100, 2) if mean1975 is not None else 0\n",
    "    percentage2020 = round((mean2020 / 10000) * 100, 2) if mean2020 is not None else 0\n",
    "\n",
    "    # Calculate the percentage change\n",
    "    if percentage1975 == 0:\n",
    "        percentage_change = 0\n",
    "    else:\n",
    "        percentage_change = round(((percentage2020 - percentage1975) / percentage1975) * 100, 2)\n",
    "\n",
    "    return percentage1975, percentage2020, percentage_change\n",
    "\n",
    "# Process each location for different cell sizes\n",
    "for index, row in data_tx.iterrows():\n",
    "    # Process for 50km cell\n",
    "    result_50km = process_location(row, 50)\n",
    "    results[\"Built_1975_50km_percent\"].append(result_50km[0])\n",
    "    results[\"Built_2020_50km_percent\"].append(result_50km[1])\n",
    "    results[\"Percentage_Change_50km\"].append(result_50km[2])\n",
    "    \n",
    "    # Process for 10km cell\n",
    "    result_10km = process_location(row, 10)\n",
    "    results[\"Built_1975_10km_percent\"].append(result_10km[0])\n",
    "    results[\"Built_2020_10km_percent\"].append(result_10km[1])\n",
    "    results[\"Percentage_Change_10km\"].append(result_10km[2])\n",
    "    \n",
    "    # Process for 2km cell\n",
    "    result_2km = process_location(row, 2)\n",
    "    results[\"Built_1975_2km_percent\"].append(result_2km[0])\n",
    "    results[\"Built_2020_2km_percent\"].append(result_2km[1])\n",
    "    results[\"Percentage_Change_2km\"].append(result_2km[2])\n",
    "\n",
    "# Add the results to the dataframe\n",
    "for key, value in results.items():\n",
    "    data_tx[key] = value\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "output_csv_path = 'updated_stations_data_tx.csv'\n",
    "data_tx.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Processing is complete and results have been written to the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b638bbbf-168a-4333-954e-9f68bb3cbd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27519\n",
      "starting batch   2000    2024-07-03 11:16:28\n",
      "starting batch   2100    2024-07-03 11:18:45\n",
      "starting batch   2200    2024-07-03 11:20:38\n",
      "starting batch   2300    2024-07-03 11:23:03\n",
      "starting batch   2400    2024-07-03 11:26:03\n",
      "starting batch   2500    2024-07-03 11:29:20\n",
      "starting batch   2600    2024-07-03 11:33:30\n",
      "starting batch   2700    2024-07-03 11:36:57\n",
      "starting batch   2800    2024-07-03 11:40:09\n",
      "starting batch   2900    2024-07-03 11:43:11\n",
      "starting batch   3000    2024-07-03 11:46:10\n",
      "starting batch   3100    2024-07-03 11:48:57\n",
      "starting batch   3200    2024-07-03 11:51:47\n",
      "starting batch   3300    2024-07-03 11:54:54\n",
      "starting batch   3400    2024-07-03 11:58:04\n",
      "starting batch   3500    2024-07-03 12:01:09\n",
      "starting batch   3600    2024-07-03 12:03:59\n",
      "starting batch   3700    2024-07-03 12:07:02\n",
      "starting batch   3800    2024-07-03 12:09:54\n",
      "starting batch   3900    2024-07-03 12:12:52\n",
      "starting batch   4000    2024-07-03 12:15:37\n",
      "starting batch   4100    2024-07-03 12:18:42\n",
      "starting batch   4200    2024-07-03 12:21:34\n",
      "starting batch   4300    2024-07-03 12:24:29\n",
      "starting batch   4400    2024-07-03 12:27:45\n",
      "starting batch   4500    2024-07-03 12:31:03\n",
      "starting batch   4600    2024-07-03 12:34:02\n",
      "starting batch   4700    2024-07-03 12:36:59\n",
      "starting batch   4800    2024-07-03 12:40:02\n",
      "starting batch   4900    2024-07-03 12:43:05\n",
      "starting batch   5000    2024-07-03 12:46:10\n",
      "starting batch   5100    2024-07-03 12:49:09\n",
      "starting batch   5200    2024-07-03 12:52:11\n",
      "starting batch   5300    2024-07-03 12:55:16\n",
      "starting batch   5400    2024-07-03 12:58:09\n",
      "starting batch   5500    2024-07-03 13:01:03\n",
      "starting batch   5600    2024-07-03 13:04:07\n",
      "starting batch   5700    2024-07-03 13:07:07\n",
      "starting batch   5800    2024-07-03 13:11:21\n",
      "starting batch   5900    2024-07-03 13:14:22\n",
      "starting batch   6000    2024-07-03 13:18:04\n",
      "starting batch   6100    2024-07-03 13:25:53\n",
      "starting batch   6200    2024-07-03 13:33:14\n",
      "starting batch   6300    2024-07-03 13:36:27\n",
      "starting batch   6400    2024-07-03 13:39:41\n",
      "starting batch   6500    2024-07-03 13:42:48\n",
      "starting batch   6600    2024-07-03 13:45:52\n",
      "starting batch   6700    2024-07-03 13:48:49\n",
      "starting batch   6800    2024-07-03 13:52:05\n",
      "starting batch   6900    2024-07-03 13:55:18\n",
      "starting batch   7000    2024-07-03 13:58:32\n",
      "starting batch   7100    2024-07-03 14:01:57\n",
      "starting batch   7200    2024-07-03 14:05:11\n",
      "starting batch   7300    2024-07-03 14:08:18\n",
      "starting batch   7400    2024-07-03 14:11:32\n",
      "starting batch   7500    2024-07-03 14:14:34\n",
      "starting batch   7600    2024-07-03 14:17:42\n",
      "starting batch   7700    2024-07-03 14:20:55\n",
      "starting batch   7800    2024-07-03 14:24:16\n",
      "starting batch   7900    2024-07-03 14:27:38\n",
      "starting batch   8000    2024-07-03 14:30:49\n",
      "starting batch   8100    2024-07-03 14:34:05\n",
      "starting batch   8200    2024-07-03 14:37:22\n",
      "starting batch   8300    2024-07-03 14:40:36\n",
      "starting batch   8400    2024-07-03 14:43:41\n",
      "starting batch   8500    2024-07-03 14:46:50\n",
      "starting batch   8600    2024-07-03 14:50:00\n",
      "starting batch   8700    2024-07-03 14:52:55\n",
      "starting batch   8800    2024-07-03 14:55:48\n",
      "starting batch   8900    2024-07-03 14:58:42\n",
      "starting batch   9000    2024-07-03 15:01:36\n",
      "starting batch   9100    2024-07-03 15:04:28\n",
      "starting batch   9200    2024-07-03 15:07:22\n",
      "starting batch   9300    2024-07-03 15:10:13\n",
      "starting batch   9400    2024-07-03 15:13:06\n",
      "starting batch   9500    2024-07-03 15:16:42\n",
      "starting batch   9600    2024-07-03 15:20:04\n",
      "starting batch   9700    2024-07-03 15:23:12\n",
      "starting batch   9800    2024-07-03 15:26:15\n",
      "starting batch   9900    2024-07-03 15:29:09\n",
      "starting batch   10000    2024-07-03 15:32:10\n",
      "starting batch   10100    2024-07-03 15:35:06\n",
      "starting batch   10200    2024-07-03 15:38:07\n",
      "starting batch   10300    2024-07-03 15:41:03\n",
      "starting batch   10400    2024-07-03 15:43:57\n",
      "starting batch   10500    2024-07-03 15:47:00\n",
      "starting batch   10600    2024-07-03 15:50:03\n",
      "starting batch   10700    2024-07-03 15:53:03\n",
      "starting batch   10800    2024-07-03 15:55:56\n",
      "starting batch   10900    2024-07-03 15:58:59\n",
      "starting batch   11000    2024-07-03 16:02:01\n",
      "starting batch   11100    2024-07-03 16:04:59\n",
      "starting batch   11200    2024-07-03 16:07:53\n",
      "starting batch   11300    2024-07-03 16:10:47\n",
      "starting batch   11400    2024-07-03 16:13:52\n",
      "starting batch   11500    2024-07-03 16:16:54\n",
      "starting batch   11600    2024-07-03 16:20:04\n",
      "starting batch   11700    2024-07-03 16:23:18\n",
      "starting batch   11800    2024-07-03 16:26:34\n",
      "starting batch   11900    2024-07-03 16:29:35\n",
      "starting batch   12000    2024-07-03 16:32:25\n",
      "starting batch   12100    2024-07-03 16:35:14\n",
      "starting batch   12200    2024-07-03 16:38:03\n",
      "starting batch   12300    2024-07-03 16:40:56\n",
      "starting batch   12400    2024-07-03 16:43:44\n",
      "starting batch   12500    2024-07-03 16:46:34\n",
      "starting batch   12600    2024-07-03 16:49:22\n",
      "starting batch   12700    2024-07-03 16:52:04\n",
      "starting batch   12800    2024-07-03 16:54:54\n",
      "starting batch   12900    2024-07-03 16:57:41\n",
      "starting batch   13000    2024-07-03 17:00:31\n",
      "starting batch   13100    2024-07-03 17:03:25\n",
      "starting batch   13200    2024-07-03 17:06:14\n",
      "starting batch   13300    2024-07-03 17:09:14\n",
      "starting batch   13400    2024-07-03 17:12:04\n",
      "starting batch   13500    2024-07-03 17:14:55\n",
      "starting batch   13600    2024-07-03 17:18:13\n",
      "starting batch   13700    2024-07-03 17:21:15\n",
      "starting batch   13800    2024-07-03 17:24:07\n",
      "starting batch   13900    2024-07-03 17:26:55\n",
      "starting batch   14000    2024-07-03 17:29:41\n",
      "starting batch   14100    2024-07-03 17:32:28\n",
      "starting batch   14200    2024-07-03 17:35:27\n",
      "starting batch   14300    2024-07-03 17:38:26\n",
      "starting batch   14400    2024-07-03 17:41:39\n",
      "starting batch   14500    2024-07-03 17:45:05\n",
      "starting batch   14600    2024-07-03 17:48:19\n",
      "starting batch   14700    2024-07-03 17:51:56\n",
      "starting batch   14800    2024-07-03 17:55:24\n",
      "starting batch   14900    2024-07-03 17:58:56\n",
      "starting batch   15000    2024-07-03 18:02:30\n",
      "starting batch   15100    2024-07-03 18:05:51\n",
      "starting batch   15200    2024-07-03 18:09:20\n",
      "starting batch   15300    2024-07-03 18:12:53\n",
      "starting batch   15400    2024-07-03 18:16:04\n",
      "starting batch   15500    2024-07-03 18:19:41\n",
      "starting batch   15600    2024-07-03 18:22:39\n",
      "starting batch   15700    2024-07-03 18:26:06\n",
      "starting batch   15800    2024-07-03 18:29:22\n",
      "starting batch   15900    2024-07-03 18:32:23\n",
      "starting batch   16000    2024-07-03 18:35:25\n",
      "starting batch   16100    2024-07-03 18:39:31\n",
      "starting batch   16200    2024-07-03 18:42:40\n",
      "starting batch   16300    2024-07-03 18:45:45\n",
      "starting batch   16400    2024-07-03 18:49:03\n",
      "starting batch   16500    2024-07-03 18:52:09\n",
      "starting batch   16600    2024-07-03 18:55:10\n",
      "starting batch   16700    2024-07-03 18:58:08\n",
      "starting batch   16800    2024-07-03 19:01:23\n",
      "starting batch   16900    2024-07-03 19:04:32\n",
      "starting batch   17000    2024-07-03 19:07:42\n",
      "starting batch   17100    2024-07-03 19:10:39\n",
      "starting batch   17200    2024-07-03 19:13:44\n",
      "starting batch   17300    2024-07-03 19:16:54\n",
      "starting batch   17400    2024-07-03 19:20:28\n",
      "starting batch   17500    2024-07-03 19:23:34\n",
      "starting batch   17600    2024-07-03 19:26:26\n",
      "starting batch   17700    2024-07-03 19:29:22\n",
      "starting batch   17800    2024-07-03 19:32:19\n",
      "starting batch   17900    2024-07-03 19:35:21\n",
      "starting batch   18000    2024-07-03 19:38:13\n",
      "starting batch   18100    2024-07-03 19:41:14\n",
      "starting batch   18200    2024-07-03 19:44:20\n",
      "starting batch   18300    2024-07-03 19:47:13\n",
      "starting batch   18400    2024-07-03 19:50:02\n",
      "starting batch   18500    2024-07-03 19:52:49\n",
      "starting batch   18600    2024-07-03 19:55:44\n",
      "starting batch   18700    2024-07-03 19:58:35\n",
      "starting batch   18800    2024-07-03 20:01:33\n",
      "starting batch   18900    2024-07-03 20:04:41\n",
      "starting batch   19000    2024-07-03 20:07:55\n",
      "starting batch   19100    2024-07-03 20:10:52\n",
      "starting batch   19200    2024-07-03 20:13:53\n",
      "starting batch   19300    2024-07-03 20:17:12\n",
      "starting batch   19400    2024-07-03 20:21:03\n",
      "starting batch   19500    2024-07-03 20:24:09\n",
      "starting batch   19600    2024-07-03 20:27:29\n",
      "starting batch   19700    2024-07-03 20:30:41\n",
      "starting batch   19800    2024-07-03 20:33:51\n",
      "starting batch   19900    2024-07-03 20:37:14\n",
      "starting batch   20000    2024-07-03 20:40:48\n",
      "starting batch   20100    2024-07-03 20:44:19\n",
      "starting batch   20200    2024-07-03 20:47:37\n",
      "starting batch   20300    2024-07-03 20:50:53\n",
      "starting batch   20400    2024-07-03 20:54:02\n",
      "starting batch   20500    2024-07-03 20:57:15\n",
      "starting batch   20600    2024-07-03 21:00:27\n",
      "starting batch   20700    2024-07-03 21:03:53\n",
      "starting batch   20800    2024-07-03 21:07:23\n",
      "starting batch   20900    2024-07-03 21:11:15\n",
      "starting batch   21000    2024-07-03 21:15:13\n",
      "starting batch   21100    2024-07-03 21:19:28\n",
      "starting batch   21200    2024-07-03 21:22:45\n",
      "starting batch   21300    2024-07-03 21:26:04\n",
      "starting batch   21400    2024-07-03 21:29:21\n",
      "starting batch   21500    2024-07-03 21:32:50\n",
      "starting batch   21600    2024-07-03 21:36:18\n",
      "starting batch   21700    2024-07-03 21:39:29\n",
      "starting batch   21800    2024-07-03 21:42:43\n",
      "starting batch   21900    2024-07-03 21:47:10\n",
      "starting batch   22000    2024-07-03 21:52:05\n",
      "starting batch   22100    2024-07-03 21:55:24\n",
      "starting batch   22200    2024-07-03 21:58:20\n",
      "starting batch   22300    2024-07-03 22:01:13\n",
      "starting batch   22400    2024-07-03 22:04:10\n",
      "starting batch   22500    2024-07-03 22:07:05\n",
      "starting batch   22600    2024-07-03 22:09:55\n",
      "starting batch   22700    2024-07-03 22:12:47\n",
      "starting batch   22800    2024-07-03 22:15:39\n",
      "starting batch   22900    2024-07-03 22:19:15\n",
      "starting batch   23000    2024-07-03 22:22:02\n",
      "starting batch   23100    2024-07-03 22:24:47\n",
      "starting batch   23200    2024-07-03 22:27:32\n",
      "starting batch   23300    2024-07-03 22:30:17\n",
      "starting batch   23400    2024-07-03 22:33:02\n",
      "starting batch   23500    2024-07-03 22:35:45\n",
      "starting batch   23600    2024-07-03 22:38:31\n",
      "starting batch   23700    2024-07-03 22:41:12\n",
      "starting batch   23800    2024-07-03 22:43:56\n",
      "starting batch   23900    2024-07-03 22:46:42\n",
      "starting batch   24000    2024-07-03 22:49:24\n",
      "starting batch   24100    2024-07-03 22:52:06\n",
      "starting batch   24200    2024-07-03 22:54:49\n",
      "starting batch   24300    2024-07-03 22:57:30\n",
      "starting batch   24400    2024-07-03 23:00:14\n",
      "starting batch   24500    2024-07-03 23:03:03\n",
      "starting batch   24600    2024-07-03 23:05:50\n",
      "starting batch   24700    2024-07-03 23:08:37\n",
      "starting batch   24800    2024-07-03 23:11:26\n",
      "starting batch   24900    2024-07-03 23:14:14\n",
      "starting batch   25000    2024-07-03 23:17:39\n",
      "starting batch   25100    2024-07-03 23:20:58\n",
      "starting batch   25200    2024-07-03 23:23:41\n",
      "starting batch   25300    2024-07-03 23:26:26\n",
      "starting batch   25400    2024-07-03 23:29:09\n",
      "starting batch   25500    2024-07-03 23:31:52\n",
      "starting batch   25600    2024-07-03 23:34:35\n",
      "starting batch   25700    2024-07-03 23:37:17\n",
      "starting batch   25800    2024-07-03 23:40:02\n",
      "starting batch   25900    2024-07-03 23:42:45\n",
      "starting batch   26000    2024-07-03 23:45:31\n",
      "starting batch   26100    2024-07-03 23:48:15\n",
      "starting batch   26200    2024-07-03 23:50:59\n",
      "starting batch   26300    2024-07-03 23:53:50\n",
      "starting batch   26400    2024-07-03 23:56:37\n",
      "starting batch   26500    2024-07-03 23:59:24\n",
      "starting batch   26600    2024-07-04 00:02:12\n",
      "starting batch   26700    2024-07-04 00:05:08\n",
      "starting batch   26800    2024-07-04 00:08:03\n",
      "starting batch   26900    2024-07-04 00:11:01\n",
      "starting batch   27000    2024-07-04 00:13:49\n",
      "starting batch   27100    2024-07-04 00:17:06\n",
      "starting batch   27200    2024-07-04 00:21:05\n",
      "starting batch   27300    2024-07-04 00:24:01\n",
      "starting batch   27400    2024-07-04 00:26:57\n",
      "starting batch   27500    2024-07-04 00:29:57\n",
      "Processing is complete and results have been written to the CSV file.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import re\n",
    "import time, random\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Earth Engine module.\n",
    "# ee.Authenticate()\n",
    "ee.Initialize()\n",
    "\n",
    "# URL of the data\n",
    "url = 'https://data.giss.nasa.gov/gistemp/station_data_v4_globe/v4.temperature.inv.txt'\n",
    "\n",
    "# Fetch the data from the URL\n",
    "response = requests.get(url)\n",
    "data_text = response.text\n",
    "\n",
    "# Clean up the data format\n",
    "data_text = data_text.replace(\"Station Name\", \"Station\")\n",
    "data_text = re.sub(r\"[ \\t]+\", \";\", data_text)\n",
    "\n",
    "while ';;' in data_text:\n",
    "    data_text = data_text.replace(\";;\", \";\")\n",
    "\n",
    "data_lines = data_text.split('\\n')\n",
    "data_lines = [line.rstrip(';') for line in data_lines]\n",
    "\n",
    "cleaned_data_text = \"\\n\".join(data_lines)\n",
    "\n",
    "if not cleaned_data_text.startswith(\"ID;Lat;Lon;Elev-m;Station;BI\"):\n",
    "    cleaned_data_text = cleaned_data_text.replace(\"D;Lat;Lon;Elev-m;Station;BI\", \"ID;Lat;Lon;Elev-m;Station;BI\", 1)\n",
    "\n",
    "data_io = StringIO(cleaned_data_text)\n",
    "\n",
    "valid_rows = []\n",
    "\n",
    "for line in data_io:\n",
    "    fields = line.split(';')\n",
    "    if len(fields) == 6:\n",
    "        valid_rows.append(fields)\n",
    "\n",
    "data = pd.DataFrame(valid_rows, columns=[\"ID\", \"Lat\", \"Lon\", \"Elev-m\", \"Station\", \"BI\"])\n",
    "\n",
    "data['Lat'] = pd.to_numeric(data['Lat'], errors='coerce')\n",
    "data['Lon'] = pd.to_numeric(data['Lon'], errors='coerce')\n",
    "data['BI'] = pd.to_numeric(data['BI'], errors='coerce')\n",
    "\n",
    "data = data.dropna(subset=['Lat', 'Lon', 'BI'])\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = {\n",
    "    \"ID\": [],\n",
    "    \"Station\": [],\n",
    "    \"BI\": [],\n",
    "    \"Built_1975_50km_percent\": [],\n",
    "    \"Built_2020_50km_percent\": [],\n",
    "    \"Percentage_Change_50km\": [],\n",
    "    \"Built_1975_10km_percent\": [],\n",
    "    \"Built_2020_10km_percent\": [],\n",
    "    \"Percentage_Change_10km\": [],\n",
    "    \"Built_1975_2km_percent\": [],\n",
    "    \"Built_2020_2km_percent\": [],\n",
    "    \"Percentage_Change_2km\": []\n",
    "}\n",
    "\n",
    "# Function to process location and calculate built-up surface percentages\n",
    "def process_location(row, sizeKm):\n",
    "    latitude = row['Lat']\n",
    "    longitude = row['Lon']\n",
    "\n",
    "    # Define a point for the center of the rectangle at the specified coordinates\n",
    "    centerPoint = ee.Geometry.Point([longitude, latitude])\n",
    "\n",
    "    # Create a bounding box around the center point\n",
    "    halfSideLength = (sizeKm / 2) * 1000  # Convert km to meters\n",
    "    cell = centerPoint.buffer(halfSideLength).bounds()\n",
    "\n",
    "    # Load the built-up surface images for 1975 and 2020 from the JRC GHSL dataset\n",
    "    image_1975 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/1975').select('built_surface')\n",
    "    image_2020 = ee.Image('JRC/GHSL/P2023A/GHS_BUILT_S/2020').select('built_surface')\n",
    "\n",
    "    # Clip the built-up images to the cell\n",
    "    built_1975_clipped = image_1975.clip(cell)\n",
    "    built_2020_clipped = image_2020.clip(cell)\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 1975\n",
    "    mean1975 = built_1975_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Calculate the average built-up value for the cell in 2020\n",
    "    mean2020 = built_2020_clipped.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=cell,\n",
    "        scale=30,\n",
    "        maxPixels=1e9\n",
    "    ).get('built_surface').getInfo()\n",
    "\n",
    "    # Normalize to percentage of the area (1% = 10,000 square meters per hectare)\n",
    "    percentage1975 = round((mean1975 / 10000) * 100, 2) if mean1975 is not None else 0\n",
    "    percentage2020 = round((mean2020 / 10000) * 100, 2) if mean2020 is not None else 0\n",
    "\n",
    "    # Calculate the percentage change\n",
    "    if percentage1975 == 0:\n",
    "        percentage_change = 0\n",
    "    else:\n",
    "        percentage_change = round(((percentage2020 - percentage1975) / percentage1975) * 100, 2)\n",
    "\n",
    "    return percentage1975, percentage2020, percentage_change\n",
    "\n",
    "# Function to process a batch of locations\n",
    "def process_batch(batch_data):\n",
    "    for index, row in batch_data.iterrows():\n",
    "        # Process for 50km cell\n",
    "        result_50km = process_location(row, 50)\n",
    "        results[\"ID\"].append(row[\"ID\"])\n",
    "        results[\"Station\"].append(row[\"Station\"])\n",
    "        results[\"BI\"].append(row[\"BI\"])\n",
    "        results[\"Built_1975_50km_percent\"].append(result_50km[0])\n",
    "        results[\"Built_2020_50km_percent\"].append(result_50km[1])\n",
    "        results[\"Percentage_Change_50km\"].append(result_50km[2])\n",
    "        \n",
    "        # Process for 10km cell\n",
    "        result_10km = process_location(row, 10)\n",
    "        results[\"Built_1975_10km_percent\"].append(result_10km[0])\n",
    "        results[\"Built_2020_10km_percent\"].append(result_10km[1])\n",
    "        results[\"Percentage_Change_10km\"].append(result_10km[2])\n",
    "        \n",
    "        # Process for 2km cell\n",
    "        result_2km = process_location(row, 2)\n",
    "        results[\"Built_1975_2km_percent\"].append(result_2km[0])\n",
    "        results[\"Built_2020_2km_percent\"].append(result_2km[1])\n",
    "        results[\"Percentage_Change_2km\"].append(result_2km[2])\n",
    "\n",
    "    # Create a DataFrame for the batch results\n",
    "    batch_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the batch results to a CSV file\n",
    "    batch_df.to_csv('updated_stations_data_tx.csv', mode='a', index=False, header=not pd.io.common.file_exists('updated_stations_data_tx.csv'))\n",
    "\n",
    "    # Clear the results for the next batch\n",
    "    for key in results.keys():\n",
    "        results[key].clear()\n",
    "\n",
    "# Process the data in batches of 1000\n",
    "batch_size = 100\n",
    "print(len(data))\n",
    "for start_index in range(2000, len(data), batch_size):\n",
    "    print (\"starting batch  \", start_index, \"  \", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    end_index = min(start_index + batch_size, len(data))\n",
    "    batch_data = data.iloc[start_index:end_index]\n",
    "    process_batch(batch_data)\n",
    "    #sleep_time = random.randint(20, 100)\n",
    "    #time.sleep(sleep_time)  # Wait for 1 minute before processing the next batch\n",
    "\n",
    "print(\"Processing is complete and results have been written to the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "428cf84a-f13b-46e7-893c-bf5e409bea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in original data: Index(['ID', 'Lat', 'Lon', 'Elev-m', 'Station', 'BI'], dtype='object')\n",
      "Columns in existing data: Index(['ID', 'Station', 'BI', 'Built_1975_50km_percent',\n",
      "       'Built_2020_50km_percent', 'Percentage_Change_50km',\n",
      "       'Built_1975_10km_percent', 'Built_2020_10km_percent',\n",
      "       'Percentage_Change_10km', 'Built_1975_2km_percent',\n",
      "       'Built_2020_2km_percent', 'Percentage_Change_2km'],\n",
      "      dtype='object')\n",
      "Columns added and CSV file updated.\n",
      "Columns in the merged data: Index(['ID', 'Station', 'BI', 'Built_1975_50km_percent',\n",
      "       'Built_2020_50km_percent', 'Percentage_Change_50km',\n",
      "       'Built_1975_10km_percent', 'Built_2020_10km_percent',\n",
      "       'Percentage_Change_10km', 'Built_1975_2km_percent',\n",
      "       'Built_2020_2km_percent', 'Percentage_Change_2km', 'Lat', 'Lon',\n",
      "       'Elev-m', 'Original_BI'],\n",
      "      dtype='object')\n",
      "Columns reordered and CSV file saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "# URL of the data\n",
    "url = 'https://data.giss.nasa.gov/gistemp/station_data_v4_globe/v4.temperature.inv.txt'\n",
    "\n",
    "# Fetch the data from the URL\n",
    "response = requests.get(url)\n",
    "data_text = response.text\n",
    "\n",
    "# Clean up the data format\n",
    "data_text = data_text.replace(\"Station Name\", \"Station\")\n",
    "data_text = re.sub(r\"[ \\t]+\", \";\", data_text)\n",
    "\n",
    "while ';;' in data_text:\n",
    "    data_text = data_text.replace(\";;\", \";\")\n",
    "\n",
    "data_lines = data_text.split('\\n')\n",
    "data_lines = [line.rstrip(';') for line in data_lines]\n",
    "\n",
    "cleaned_data_text = \"\\n\".join(data_lines)\n",
    "\n",
    "#if not cleaned_data_text.startswith(\"ID;Lat;Lon;Elev-m;Station;BI\"):\n",
    "#    cleaned_data_text = cleaned_data_text.replace(\"D;Lat;Lon;Elev-m;Station;BI\", \"ID;Lat;Lon;Elev-m;Station;BI\", 1)\n",
    "\n",
    "data_io = StringIO(cleaned_data_text)\n",
    "\n",
    "valid_rows = []\n",
    "\n",
    "for line in data_io:\n",
    "    fields = line.split(';')\n",
    "    if len(fields) == 6:\n",
    "        valid_rows.append(fields)\n",
    "\n",
    "data_original = pd.DataFrame(valid_rows, columns=[\"ID\", \"Lat\", \"Lon\", \"Elev-m\", \"Station\", \"BI\"])\n",
    "\n",
    "# Convert the numeric columns from strings to appropriate numeric types\n",
    "data_original['Lat'] = pd.to_numeric(data_original['Lat'], errors='coerce')\n",
    "data_original['Lon'] = pd.to_numeric(data_original['Lon'], errors='coerce')\n",
    "data_original['Elev-m'] = pd.to_numeric(data_original['Elev-m'], errors='coerce')\n",
    "data_original['BI'] = pd.to_numeric(data_original['BI'], errors='coerce')\n",
    "\n",
    "# Load the existing CSV file\n",
    "output_csv_path = 'updated_stations_data_tx.csv'\n",
    "data_existing = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Debug: Print columns before merging\n",
    "print(\"Columns in original data:\", data_original.columns)\n",
    "print(\"Columns in existing data:\", data_existing.columns)\n",
    "\n",
    "# Ensure 'ID' column is present in both DataFrames\n",
    "if 'ID' not in data_existing.columns:\n",
    "    print(\"'ID' column not found in existing data. Please check the input file.\")\n",
    "else:\n",
    "    # Merge the original data with the existing data on the \"ID\" column\n",
    "    data_merged = pd.merge(data_existing, data_original[[\"ID\", \"Lat\", \"Lon\", \"Elev-m\", \"BI\"]], on=\"ID\", how=\"left\")\n",
    "\n",
    "    # Rename columns to avoid confusion\n",
    "    data_merged = data_merged.rename(columns={'BI_x': 'BI', 'BI_y': 'Original_BI'})\n",
    "\n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    data_merged.to_csv('updated_stations_data_tx_all.csv', index=False)\n",
    "\n",
    "    print(\"Columns added and CSV file updated.\")\n",
    "\n",
    "# Read the result file and check columns\n",
    "data_reordered = pd.read_csv('updated_stations_data_tx_all.csv')\n",
    "\n",
    "# Debug: Print columns after merging and saving\n",
    "print(\"Columns in the merged data:\", data_reordered.columns)\n",
    "\n",
    "# Reorder columns if they all exist\n",
    "columns_order = [\n",
    "    \"ID\", \"Station\", \"Lat\", \"Lon\", \"Elev-m\", \"BI\",\n",
    "    \"Built_1975_50km_percent\", \"Built_2020_50km_percent\", \"Percentage_Change_50km\",\n",
    "    \"Built_1975_10km_percent\", \"Built_2020_10km_percent\", \"Percentage_Change_10km\",\n",
    "    \"Built_1975_2km_percent\", \"Built_2020_2km_percent\", \"Percentage_Change_2km\"\n",
    "]\n",
    "\n",
    "missing_columns = [col for col in columns_order if col not in data_reordered.columns]\n",
    "if missing_columns:\n",
    "    print(\"Missing columns in the merged data:\", missing_columns)\n",
    "else:\n",
    "    data_reordered = data_reordered[columns_order]\n",
    "    data_reordered.to_csv('reordered_stations_data_tx_all.csv', index=False)\n",
    "    print(\"Columns reordered and CSV file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4e9a2db-d60a-443b-ad33-d254cd7290da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns reordered and CSV file saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV file\n",
    "input_csv_path = 'updated_stations_data_tx_all.csv'\n",
    "data_merged = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Define the desired column order\n",
    "columns_order = [\n",
    "    \"ID\", \"Station\", \"Lat\", \"Lon\", \"Elev-m\", \"BI\",\n",
    "    \"Built_1975_50km_percent\", \"Built_2020_50km_percent\", \"Percentage_Change_50km\",\n",
    "    \"Built_1975_10km_percent\", \"Built_2020_10km_percent\", \"Percentage_Change_10km\",\n",
    "    \"Built_1975_2km_percent\", \"Built_2020_2km_percent\", \"Percentage_Change_2km\"\n",
    "]\n",
    "\n",
    "# Reorder the columns\n",
    "data_merged = data_merged[columns_order]\n",
    "\n",
    "# Save the reordered DataFrame to a new CSV file\n",
    "output_csv_path = 'reordered_stations_data_tx_all.csv'\n",
    "data_merged.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Columns reordered and CSV file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1b0b925-41e8-4223-a1f3-67a02706a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched stations: 139\n",
      "Matched USCRN stations:\n",
      "Original Station: FAIRBANKS_11_NE, USCRN Station: AK_Fairbanks_11_NE, ID: USW00026494\n",
      "Original Station: PAXSON, USCRN Station: AK_Glennallen_64_N, ID: USC00507097\n",
      "Original Station: KENAI_29_ENE, USCRN Station: AK_Kenai_29_ENE, ID: USW00026563\n",
      "Original Station: METLAKATLA_6_S, USCRN Station: AK_Metlakatla_6_S, ID: USW00025381\n",
      "Original Station: PORT_ALSWORTH, USCRN Station: AK_Port_Alsworth_1_SW, ID: USC00507570\n",
      "Original Station: SAND_POINT_1_ENE, USCRN Station: AK_Sand_Point_1_ENE, ID: USW00025630\n",
      "Original Station: SITKA_1_NE, USCRN Station: AK_Sitka_1_NE, ID: USW00025379\n",
      "Original Station: YAKUTAT_STATE_AP, USCRN Station: AK_Yakutat_3_SSE, ID: USW00025339\n",
      "Original Station: BREWTON_3_ENE, USCRN Station: AL_Brewton_3_NNE, ID: USC00011080\n",
      "Original Station: CLANTON_2_NE, USCRN Station: AL_Clanton_2_NE, ID: USW00063891\n",
      "Original Station: COURTLAND_2_WSW, USCRN Station: AL_Courtland_2_WSW, ID: USW00063868\n",
      "Original Station: CULLMAN_NAHS, USCRN Station: AL_Cullman_3_ENE, ID: USC00012096\n",
      "Original Station: FAIRHOPE_2_NE, USCRN Station: AL_Fairhope_3_NE, ID: USC00012813\n",
      "Original Station: SAND_MT_SUBSTN, USCRN Station: AL_Gadsden_19_N, ID: USC00017207\n",
      "Original Station: GAINESVILLE_LOCK, USCRN Station: AL_Gainesville_2_NE, ID: USC00013160\n",
      "Original Station: GREENSBORO_2_WNW, USCRN Station: AL_Greensboro_2_WNW, ID: USW00063893\n",
      "Original Station: HIGHLAND_HOME_2_S, USCRN Station: AL_Highland_Home_2_S, ID: USW00073802\n",
      "Original Station: MUSCLE_SHOALS_2_N, USCRN Station: AL_Muscle_Shoals_2_N, ID: USW00063894\n",
      "Original Station: TUSCALOOSA_OLIVER_DAM, USCRN Station: AL_Northport_2_S, ID: USC00018385\n",
      "Original Station: RUSSELLVILLE_4_SSE, USCRN Station: AL_Russellville_4_SSE, ID: USW00063895\n",
      "Original Station: SCOTTSBORO_2_NE, USCRN Station: AL_Scottsboro_2_NE, ID: USW00063896\n",
      "Original Station: SELMA_13_WNW, USCRN Station: AL_Selma_13_WNW, ID: USW00063858\n",
      "Original Station: CRAIG_AFB, USCRN Station: AL_Selma_6_SSE, ID: USW00013850\n",
      "Original Station: TALLADEGA_10_NNE, USCRN Station: AL_Talladega_10_NNE, ID: USW00073803\n",
      "Original Station: THOMASVILLE_2_S, USCRN Station: AL_Thomasville_2_S, ID: USW00023802\n",
      "Original Station: TROY_2_W, USCRN Station: AL_Troy_2_W, ID: USW00023801\n",
      "Original Station: VALLEY_HEAD_1_SSW, USCRN Station: AL_Valley_Head_1_SSW, ID: USW00063862\n",
      "Original Station: BATESVILLE_8_WNW, USCRN Station: AR_Batesville_8_WNW, ID: USW00023904\n",
      "Original Station: ELGIN_5_S, USCRN Station: AZ_Elgin_5_S, ID: USW00053132\n",
      "Original Station: TUCSON_11_W, USCRN Station: AZ_Tucson_11_W, ID: USW00053131\n",
      "Original Station: WILLIAMS_35_NNW, USCRN Station: AZ_Williams_35_NNW, ID: USW00053155\n",
      "Original Station: YUMA_27_ENE, USCRN Station: AZ_Yuma_27_ENE, ID: USW00053154\n",
      "Original Station: BODEGA_6_WSW, USCRN Station: CA_Bodega_6_WSW, ID: USW00093245\n",
      "Original Station: FALLBROOK_5_NE, USCRN Station: CA_Fallbrook_5_NE, ID: USW00053151\n",
      "Original Station: MERCED_23_WSW, USCRN Station: CA_Merced_23_WSW, ID: USW00093243\n",
      "Original Station: REDDING_12_WNW, USCRN Station: CA_Redding_12_WNW, ID: USW00004222\n",
      "Original Station: SANTA_BARBARA_11_W, USCRN Station: CA_Santa_Barbara_11_W, ID: USW00053152\n",
      "Original Station: STOVEPIPE_WELLS_1_SW, USCRN Station: CA_Stovepipe_Wells_1_SW, ID: USW00053139\n",
      "Original Station: CRANE_FLAT_LOOKOUT_CALIFORNIA, USCRN Station: CA_Yosemite_Village_12_W, ID: USR0000CCRA\n",
      "Original Station: NIWOT, USCRN Station: CO_Boulder_14_W, ID: USS0005J42S\n",
      "Original Station: CORTEZ_8_SE, USCRN Station: CO_Cortez_8_SE, ID: USW00003061\n",
      "Original Station: DINOSAUR_NATL_MONUMNT, USCRN Station: CO_Dinosaur_2_E, ID: USC00052286\n",
      "Original Station: LA_JUNTA_17_WSW, USCRN Station: CO_La_Junta_17_WSW, ID: USW00003063\n",
      "Original Station: BLACK_CANYON_COLORADO, USCRN Station: CO_Montrose_11_ENE, ID: USR0000CBLA\n",
      "Original Station: NUNN_7_NNE, USCRN Station: CO_Nunn_7_NNE, ID: USW00094074\n",
      "Original Station: EVERGLADES_CITY_5_NE, USCRN Station: FL_Everglades_City_5_NE, ID: USW00092826\n",
      "Original Station: SEBRING_23_SSE, USCRN Station: FL_Sebring_23_SSE, ID: USW00092827\n",
      "Original Station: TITUSVILLE_7_E, USCRN Station: FL_Titusville_7_E, ID: USW00092821\n",
      "Original Station: BRUNSWICK_23_S, USCRN Station: GA_Brunswick_23_S, ID: USW00063856\n",
      "Original Station: HOGGARDS_MILL, USCRN Station: GA_Newton_11_SW, ID: USC00094388\n",
      "Original Station: NEWTON_8_W, USCRN Station: GA_Newton_8_W, ID: USW00063828\n",
      "Original Station: WATKINSVILLE_5_SSE, USCRN Station: GA_Watkinsville_5_SSE, ID: USW00063850\n",
      "Original Station: HILO_5_S, USCRN Station: HI_Hilo_5_S, ID: USW00021515\n",
      "Original Station: MAUNA_LOA_SLOPE_OBS_39, USCRN Station: HI_Mauna_Loa_5_NNE, ID: USC00516198\n",
      "Original Station: DES_MOINES_17_E, USCRN Station: IA_Des_Moines_17_E, ID: USW00054902\n",
      "Original Station: CRATERS_OF_THE_MOON, USCRN Station: ID_Arco_17_SW, ID: USC00102260\n",
      "Original Station: REYNOLDS, USCRN Station: ID_Murphy_10_W, ID: USC00107648\n",
      "Original Station: CHAMPAIGN_9_SW, USCRN Station: IL_Champaign_9_SW, ID: USW00054808\n",
      "Original Station: SHABBONA_5_NNE, USCRN Station: IL_Shabbona_5_NNE, ID: USW00054811\n",
      "Original Station: BEDFORD_5_WNW, USCRN Station: IN_Bedford_5_WNW, ID: USW00063898\n",
      "Original Station: MANHATTAN_6_SSW, USCRN Station: KS_Manhattan_6_SSW, ID: USW00053974\n",
      "Original Station: OAKLEY_19_SSW, USCRN Station: KS_Oakley_19_SSW, ID: USW00003067\n",
      "Original Station: BOWLING_GREEN_21_NNE, USCRN Station: KY_Bowling_Green_21_NNE, ID: USW00063849\n",
      "Original Station: VERSAILLES_3_NNW, USCRN Station: KY_Versailles_3_NNW, ID: USW00063838\n",
      "Original Station: ST_MARTINVILLE_3_SW, USCRN Station: LA_Lafayette_13_SE, ID: USC00168181\n",
      "Original Station: MONROE_26_N, USCRN Station: LA_Monroe_26_N, ID: USW00053961\n",
      "Original Station: LIMESTONE_LORING_AFB, USCRN Station: ME_Limestone_4_NNW, ID: USW00014623\n",
      "Original Station: OLD_TOWN_2_W, USCRN Station: ME_Old_Town_2_W, ID: USW00094644\n",
      "Original Station: CHATHAM_1_SE, USCRN Station: MI_Chatham_1_SE, ID: USW00054810\n",
      "Original Station: GAYLORD_9SSW, USCRN Station: MI_Gaylord_9_SSW, ID: USC00203099\n",
      "Original Station: GOODRIDGE_12_NNW, USCRN Station: MN_Goodridge_12_NNW, ID: USW00004994\n",
      "Original Station: SANDSTONE_6_W, USCRN Station: MN_Sandstone_6_W, ID: USW00054932\n",
      "Original Station: CHILLICOTHE_22_ENE, USCRN Station: MO_Chillicothe_22_ENE, ID: USW00013301\n",
      "Original Station: JOPLIN_24_N, USCRN Station: MO_Joplin_24_N, ID: USW00023908\n",
      "Original Station: SALEM_10_W, USCRN Station: MO_Salem_10_W, ID: USW00023909\n",
      "Original Station: HOLLY_SPRINGS_4_N, USCRN Station: MS_Holly_Springs_4_N, ID: USC00224173\n",
      "Original Station: NEWTON_5_ENE, USCRN Station: MS_Newton_5_ENE, ID: USW00063831\n",
      "Original Station: DILLON_18_WSW, USCRN Station: MT_Dillon_18_WSW, ID: USW00004137\n",
      "Original Station: UTICA_11_WSW, USCRN Station: MT_Lewistown_42_WSW, ID: USC00248495\n",
      "Original Station: WOLF_POINT_29_ENE, USCRN Station: MT_Wolf_Point_29_ENE, ID: USW00094060\n",
      "Original Station: WOLF_POINT_34_NE, USCRN Station: MT_Wolf_Point_34_NE, ID: USW00094059\n",
      "Original Station: FLETCHER_3_W, USCRN Station: NC_Asheville_13_S, ID: USC00313106\n",
      "Original Station: ASHEVILLE_8_SSW, USCRN Station: NC_Asheville_8_SSW, ID: USW00053877\n",
      "Original Station: DUKE_FOREST_NORTH_CAROLINA, USCRN Station: NC_Durham_11_W, ID: USR0000NDUK\n",
      "Original Station: JAMESTOWN_38_WSW, USCRN Station: ND_Jamestown_38_WSW, ID: USW00054937\n",
      "Original Station: MEDORA_7_E, USCRN Station: ND_Medora_7_E, ID: USW00094080\n",
      "Original Station: NORTHGATE_5_ESE, USCRN Station: ND_Northgate_5_ESE, ID: USW00094084\n",
      "Original Station: AGATE_3_E, USCRN Station: NE_Harrison_20_SSE, ID: USC00250030\n",
      "Original Station: LINCOLN_11_SW, USCRN Station: NE_Lincoln_11_SW, ID: USW00094996\n",
      "Original Station: LINCOLN_8_ENE, USCRN Station: NE_Lincoln_8_ENE, ID: USW00094995\n",
      "Original Station: WHITMAN_5_ENE, USCRN Station: NE_Whitman_5_ENE, ID: USW00094079\n",
      "Original Station: DURHAM_2_N, USCRN Station: NH_Durham_2_N, ID: USW00054794\n",
      "Original Station: DURHAM_2_SSW, USCRN Station: NH_Durham_2_SSW, ID: USW00054795\n",
      "Original Station: JORNADA_EXP_RANGE, USCRN Station: NM_Las_Cruces_20_N, ID: USC00294426\n",
      "Original Station: LOS_ALAMOS_13_W, USCRN Station: NM_Los_Alamos_13_W, ID: USW00003062\n",
      "Original Station: SOCORRO_20_N, USCRN Station: NM_Socorro_20_N, ID: USW00003048\n",
      "Original Station: LEHMAN_CAVES_NM, USCRN Station: NV_Baker_5_W, ID: USC00264514\n",
      "Original Station: SHELDON, USCRN Station: NV_Denio_52_WSW, ID: USC00267443\n",
      "Original Station: MERCURY_DESERT_ROCK_AP, USCRN Station: NV_Mercury_3_SSW, ID: USW00003160\n",
      "Original Station: ITHACA_13_E, USCRN Station: NY_Ithaca_13_E, ID: USW00064758\n",
      "Original Station: MILLBROOK_3_W, USCRN Station: NY_Millbrook_3_W, ID: USW00064756\n",
      "Original Station: GOODWELL_2_E, USCRN Station: OK_Goodwell_2_E, ID: USW00003055\n",
      "Original Station: GOODWELL_2_SE, USCRN Station: OK_Goodwell_2_SE, ID: USW00053182\n",
      "Original Station: STILLWATER_MESONET, USCRN Station: OK_Stillwater_2_W, ID: USC00348499\n",
      "Original Station: STILLWATER_5_WNW, USCRN Station: OK_Stillwater_5_WNW, ID: USW00053927\n",
      "Original Station: EGBERT_CS, USCRN Station: ON_Egbert_1_W, ID: CA00611E001\n",
      "Original Station: COOS_BAY_8_SW, USCRN Station: OR_Coos_Bay_8_SW, ID: USW00004141\n",
      "Original Station: DAYVILLE_8_NW, USCRN Station: OR_John_Day_35_WNW, ID: USC00352173\n",
      "Original Station: RILEY_10_WSW, USCRN Station: OR_Riley_10_WSW, ID: USW00004128\n",
      "Original Station: AVONDALE_2_N, USCRN Station: PA_Avondale_2_N, ID: USW00003761\n",
      "Original Station: KINGSTON, USCRN Station: RI_Kingston_1_NW, ID: USC00374266\n",
      "Original Station: KINGSTON_1_W, USCRN Station: RI_Kingston_1_W, ID: USW00054797\n",
      "Original Station: BLACKVILLE_3_W, USCRN Station: SC_Blackville_3_W, ID: USC00380764\n",
      "Original Station: MCCLELLANVILLE_7_NE, USCRN Station: SC_McClellanville_7_NE, ID: USW00003728\n",
      "Original Station: ABERDEEN_35_WNW, USCRN Station: SD_Aberdeen_35_WNW, ID: USW00054933\n",
      "Original Station: BUFFALO_13_ESE, USCRN Station: SD_Buffalo_13_ESE, ID: USW00094081\n",
      "Original Station: PIERRE_24_S, USCRN Station: SD_Pierre_24_S, ID: USW00094085\n",
      "Original Station: SIOUX_FALLS_14_NNE, USCRN Station: SD_Sioux_Falls_14_NNE, ID: USW00004990\n",
      "Original Station: CROSSVILLE_EDRESEARCH, USCRN Station: TN_Crossville_7_NW, ID: USC00402202\n",
      "Original Station: AUSTIN_33_NW, USCRN Station: TX_Austin_33_NW, ID: USW00023907\n",
      "Original Station: BRONTE_11_NNE, USCRN Station: TX_Bronte_11_NNE, ID: USW00003072\n",
      "Original Station: EDINBURG_17_NNE, USCRN Station: TX_Edinburg_17_NNE, ID: USW00012987\n",
      "Original Station: MONAHANS_6_ENE, USCRN Station: TX_Monahans_6_ENE, ID: USW00003047\n",
      "Original Station: MULESHOE_NTL_WR, USCRN Station: TX_Muleshoe_19_S, ID: USC00416137\n",
      "Original Station: PALESTINE_6_WNW, USCRN Station: TX_Palestine_6_WNW, ID: USW00053968\n",
      "Original Station: PANTHER_JUNCTION_2_N, USCRN Station: TX_Panther_Junction_2_N, ID: USW00022016\n",
      "Original Station: PORT_ARANSAS_32_NNE, USCRN Station: TX_Port_Aransas_32_NNE, ID: USW00023906\n",
      "Original Station: BRIGHAM_CITY_28_WNW, USCRN Station: UT_Brigham_City_28_WNW, ID: USW00004138\n",
      "Original Station: TORREY_7_E, USCRN Station: UT_Torrey_7_E, ID: USW00053149\n",
      "Original Station: CAPE_CHARLES_5_ENE, USCRN Station: VA_Cape_Charles_5_ENE, ID: USW00003739\n",
      "Original Station: CHARLOTTESVILLE_2_SSE, USCRN Station: VA_Charlottesville_2_SSE, ID: USW00003759\n",
      "Original Station: DARRINGTON_21_NNE, USCRN Station: WA_Darrington_21_NNE, ID: USW00004223\n",
      "Original Station: QUINAULT_4_NE, USCRN Station: WA_Quinault_4_NE, ID: USW00004237\n",
      "Original Station: SPOKANE_17_SSW, USCRN Station: WA_Spokane_17_SSW, ID: USW00004136\n",
      "Original Station: NECEDAH_5_WNW, USCRN Station: WI_Necedah_5_WNW, ID: USW00054903\n",
      "Original Station: ELKINS_21_ENE, USCRN Station: WV_Elkins_21_ENE, ID: USW00003733\n",
      "Original Station: LANDER_11_SSE, USCRN Station: WY_Lander_11_SSE, ID: USW00094078\n",
      "Original Station: MOOSE, USCRN Station: WY_Moose_1_NNE, ID: USC00486428\n",
      "Original Station: SUNDANCE_8_NNW, USCRN Station: WY_Sundance_8_NNW, ID: USW00094088\n",
      "Missing USCRN stations:\n",
      "AK_Aleknagik_1_NNE\n",
      "AK_Bethel_87_WNW\n",
      "AK_Cordova_14_ESE\n",
      "AK_Deadhorse_3_S\n",
      "AK_Galena_44_SW\n",
      "AK_Gustavus_2_NE\n",
      "AK_Huslia_27_E\n",
      "AK_King_Salmon_42_SE\n",
      "AK_Red_Dog_Mine_3_SSW\n",
      "AK_Ruby_44_ESE\n",
      "AK_Selawik_28_E\n",
      "AK_Tok_70_SE\n",
      "AK_Toolik_Lake_5_ENE\n",
      "AK_Utqiagvik_formerly_Barrow_4_ENE\n",
      "OH_Wooster_3_SSE\n",
      "\n",
      "Closest GHCN locations for missing USCRN stations:\n",
      "USCRN Station: AK_Aleknagik_1_NNE, Closest GHCN Station: DILLINGHAM_FAA_AP, Distance: 26.17 km\n",
      "USCRN Station: AK_Bethel_87_WNW, Closest GHCN Station: REINDEER_RIVER_ALASKA, Distance: 85.62 km\n",
      "USCRN Station: AK_Cordova_14_ESE, Closest GHCN Station: CORDOVA_M_K_SMITH_AP, Distance: 5.95 km\n",
      "USCRN Station: AK_Deadhorse_3_S, Closest GHCN Station: UMIAT, Distance: 167.21 km\n",
      "USCRN Station: AK_Galena_44_SW, Closest GHCN Station: KAIYUH_ALASKA, Distance: 9.62 km\n",
      "USCRN Station: AK_Gustavus_2_NE, Closest GHCN Station: GUSTAVUS, Distance: 2.38 km\n",
      "USCRN Station: AK_Huslia_27_E, Closest GHCN Station: COTTONWOOD_ALASKA, Distance: 40.90 km\n",
      "USCRN Station: AK_King_Salmon_42_SE, Closest GHCN Station: KING_SALMON, Distance: 66.71 km\n",
      "USCRN Station: AK_Red_Dog_Mine_3_SSW, Closest GHCN Station: NOATAK_ALASKA, Distance: 175.95 km\n",
      "USCRN Station: AK_Ruby_44_ESE, Closest GHCN Station: ROUND_LAKE_ALASKA, Distance: 22.51 km\n",
      "USCRN Station: AK_Selawik_28_E, Closest GHCN Station: SELAWIK_ALASKA, Distance: 6.94 km\n",
      "USCRN Station: AK_Tok_70_SE, Closest GHCN Station: ALCAN_HWY_MI_1244_ALASKA, Distance: 15.65 km\n",
      "USCRN Station: AK_Toolik_Lake_5_ENE, Closest GHCN Station: CHANDALAR_LAKE, Distance: 132.54 km\n",
      "USCRN Station: AK_Utqiagvik_formerly_Barrow_4_ENE, Closest GHCN Station: UMIAT, Distance: 274.70 km\n",
      "USCRN Station: OH_Wooster_3_SSE, Closest GHCN Station: WOOSTER_EXP_STATION, Distance: 2.65 km\n",
      "Updated CSV file with USCRN_Y_N column saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Function to check if two locations are approximately equal\n",
    "def is_approx_equal(lat1, lon1, lat2, lon2, tol=0.01):\n",
    "    return abs(lat1 - lat2) < tol and abs(lon1 - lon2) < tol\n",
    "\n",
    "# Function to calculate the distance between two points in kilometers\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return geodesic(coord1, coord2).kilometers\n",
    "\n",
    "# Load the previously created CSV file\n",
    "previous_csv_path = 'reordered_stations_data_tx_all.csv'\n",
    "previous_data = pd.read_csv(previous_csv_path)\n",
    "\n",
    "# Load the new CSV file from GitHub\n",
    "new_csv_url = 'https://raw.githubusercontent.com/orwell2024/uscrnlib/main/extract_slides/2024stations_days.csv'\n",
    "new_data = pd.read_csv(new_csv_url)\n",
    "\n",
    "# Add the USCRN_Y_N column with default value 'N'\n",
    "previous_data['USCRN_Y_N'] = 'N'\n",
    "\n",
    "# Ensure the USCRN_Station column exists and is of type object (string)\n",
    "previous_data['USCRN_Station'] = \"\"\n",
    "\n",
    "# List to keep track of matched and missing stations\n",
    "matched_stations = []\n",
    "missing_stations = []\n",
    "\n",
    "# Iterate over the new data and check for matches in the previous data\n",
    "i = 0\n",
    "for index, row in new_data.iterrows():\n",
    "    station_name = row['Station']\n",
    "    lat = row['LATITUDE']\n",
    "    lon = row['LONGITUDE']\n",
    "    \n",
    "    # Check for approximate match in the previous data\n",
    "    match = previous_data.apply(lambda x: is_approx_equal(x['Lat'], x['Lon'], lat, lon), axis=1)\n",
    "    \n",
    "    if match.any():\n",
    "        i += 1\n",
    "        matched_index = match.idxmax()\n",
    "        original_station_name = previous_data.loc[matched_index, 'Station']\n",
    "        matched_stations.append((original_station_name, station_name, previous_data.loc[matched_index, 'ID']))\n",
    "        # Update the USCRN_Y_N column to 'Y' where there is a match\n",
    "        previous_data.loc[matched_index, 'USCRN_Y_N'] = 'Y'\n",
    "        # Add the new station name\n",
    "        previous_data.loc[matched_index, 'USCRN_Station'] = station_name\n",
    "    else:\n",
    "        missing_stations.append((station_name, lat, lon))\n",
    "\n",
    "# Print the matched stations count and missing stations\n",
    "print(f\"Total matched stations: {i}\")\n",
    "print(\"Matched USCRN stations:\")\n",
    "for original_station, uscrn_station, station_id in matched_stations:\n",
    "    print(f\"Original Station: {original_station}, USCRN Station: {uscrn_station}, ID: {station_id}\")\n",
    "\n",
    "print(\"Missing USCRN stations:\")\n",
    "for station_name, lat, lon in missing_stations:\n",
    "    print(station_name)\n",
    "\n",
    "# Find the closest GHCN location for each missing USCRN station\n",
    "print(\"\\nClosest GHCN locations for missing USCRN stations:\")\n",
    "for station_name, uscrn_lat, uscrn_lon in missing_stations:\n",
    "    min_distance = float('inf')\n",
    "    closest_ghcn_station = None\n",
    "    closest_ghcn_lat = None\n",
    "    closest_ghcn_lon = None\n",
    "    \n",
    "    for index, row in previous_data.iterrows():\n",
    "        ghcn_lat = row['Lat']\n",
    "        ghcn_lon = row['Lon']\n",
    "        distance = calculate_distance((uscrn_lat, uscrn_lon), (ghcn_lat, ghcn_lon))\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_ghcn_station = row['Station']\n",
    "            closest_ghcn_lat = ghcn_lat\n",
    "            closest_ghcn_lon = ghcn_lon\n",
    "            \n",
    "    print(f\"USCRN Station: {station_name}, Closest GHCN Station: {closest_ghcn_station}, Distance: {min_distance:.2f} km\")\n",
    "\n",
    "# Reorder columns to make USCRN_Y_N the third column\n",
    "cols = list(previous_data.columns)\n",
    "cols.insert(2, cols.pop(cols.index('USCRN_Y_N')))\n",
    "previous_data = previous_data[cols]\n",
    "\n",
    "# Save the reordered DataFrame back to the CSV file\n",
    "updated_csv_path = 'updated_stations_data_with_uscrn.csv'\n",
    "previous_data.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(\"Updated CSV file with USCRN_Y_N column saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e735295-903b-454d-b21f-4018ab7a1338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
